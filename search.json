[
  {
    "objectID": "configuration.html#configuration-class",
    "href": "configuration.html#configuration-class",
    "title": "Static Configurations",
    "section": "Configuration Class",
    "text": "Configuration Class\nThe Config class manages all configuration through a combination of default values and environment variables loaded from .env.\n\nsource\n\nConfig\n\n Config ()\n\nGlobal configuration for the thucy project\n\n\nSingleton Instance\nWe create a single global config instance that can be imported throughout the project.\n\n\nExported source\nconfig = Config()\n\n\n\nconfig.restore_defaults()\n\n\nconfig.show_config()\n\nOPENAI_API_KEY=sk-xxyy\nEXPERTS_MODEL=gpt-5-mini\nLEAD_MODEL=gpt-5\nSQL_EXPERT_MAX_TURNS=30\nSCHEMA_EXPERT_MAX_TURNS=30\nDATA_EXPERT_MAX_TURNS=30\nGENAI_SERVER_URL=http://127.0.0.1:5000\nLEAD_MAX_TURNS=40\n\n\n\n\nUsage Examples\nThe following cells demonstrate how to set and retrieve environment variables using the config system.\nCheck if variable exists before setting it:\n\nimport os\n\n\nos.getenv(\"SQL_EXPERT_MAX_TURNS\")\n\n'30'\n\n\nSet the variable and verify it‚Äôs saved:\n\nconfig.set_env_var('SQL_EXPERT_MAX_TURNS', 5)\n\n\nos.getenv(\"SQL_EXPERT_MAX_TURNS\")\n\n'5'\n\n\n\nconfig.sql_expert_max_turns\n\n5\n\n\n\nconfig.set_env_var('EXPERTS_MODEL', 'gpt-5.1')\n\n\nos.getenv(\"EXPERTS_MODEL\")\n\n'gpt-5.1'\n\n\nAccess via config object (lowercase attribute):\n\nconfig.experts_model\n\n'gpt-5.1'\n\n\nRestore to original default values:\n\nconfig.restore_defaults()\n\n\nconfig.show_config()\n\nOPENAI_API_KEY=sk-xxyy\nEXPERTS_MODEL=gpt-5-mini\nLEAD_MODEL=gpt-5\nSQL_EXPERT_MAX_TURNS=30\nSCHEMA_EXPERT_MAX_TURNS=30\nDATA_EXPERT_MAX_TURNS=30\nGENAI_SERVER_URL=http://127.0.0.1:5000\nLEAD_MAX_TURNS=40\n\n\n\nconfig.lead_max_turns\n\n40\n\n\n\nsource\n\n\nappend_to_file\n\n append_to_file (filename:str, data:pydantic.main.BaseModel,\n                 general_tag:str)\n\nAppend the attributes of a Pydantic BaseModel instance to a file within a general tag.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nfilename\nstr\nfilename to append to\n\n\ndata\nBaseModel\na Pydantic BaseModel instance with attributes to log\n\n\ngeneral_tag\nstr\na general tag to wrap around the data\n\n\n\n\nclass TestBucket(BaseModel):\n    name: str = \"TestBucket\"\n    size: int = 10\n    region: str = \"us-west-1\"\n\nappend_to_file(\"test_log.txt\", data=TestBucket(), general_tag=\"CONFIGURATION\")\n\n\nwith open(config.results_dir / \"test_log.txt\", \"r\", encoding=\"utf-8\") as f:\n    print(f.read())\n\n\n&lt;CONFIGURATION&gt;\n&lt;name&gt;\nTestBucket\n&lt;/name&gt;\n&lt;size&gt;\n10\n&lt;/size&gt;\n&lt;region&gt;\nus-west-1\n&lt;/region&gt;\n&lt;/CONFIGURATION&gt;\n\n\n\n\nos.remove(config.results_dir / \"test_log.txt\")",
    "crumbs": [
      "Static Configurations"
    ]
  },
  {
    "objectID": "toolbox.html",
    "href": "toolbox.html",
    "title": "Google‚Äôs MCP Toolbox",
    "section": "",
    "text": "source\n\nGenAIToolboxMCP\n\n GenAIToolboxMCP ()\n\nFactory for managing a shared ToolboxSyncClient and loading toolsets.\nWe first create a singleton instance of GenAIToolboxMCP.\n\n\nExported source\ngenai_mcp = GenAIToolboxMCP()\n\n\nThen, we preconfigure the tools to redirect to the toolsets with the prefix of seattle. For example, after the following command, the SQL expert agent will subscribe to the tooset seattle-sql.\n\ngenai_mcp.set_specific_toolset(\"seattle\")\n\n\ngenai_mcp.connect()\n\nLet‚Äôs load some tools and see what is going on. First, we will load sql tools:\n\nsql_tools = genai_mcp._load_raw_toolset(\"sql\")\n\nThe length of the loaded tools should be 1, as we have 1 SQL toolsets in the seattle toolset: postgres_seattle_execute_sql.\n\nlen(sql_tools)\n\n1\n\n\nLet‚Äôs run some SQL using a tool. They are directly callable!\n\nsql_tools[0](\"select * from crime_data limit 1;\")\n\n'[{\"beat\":\"W2\",\"block_address\":\"59XX BLOCK OF 45TH AVE SW\",\"latitude\":\"47.549439\",\"longitude\":\"-122.389912\",\"neighborhood\":\"MORGAN\",\"nibrs_crime_against_category\":\"PROPERTY\",\"nibrs_group_ab\":\"A\",\"nibrs_offense_code\":\"23F\",\"nibrs_offense_code_description\":\"Theft From Motor Vehicle\",\"offense_category\":\"PROPERTY CRIME\",\"offense_date\":\"2024-07-05T22:00:00Z\",\"offense_id\":57458328658,\"offense_sub_category\":\"LARCENY-THEFT\",\"precinct\":\"Southwest\",\"report_datetime\":\"2024-07-07T07:00:10Z\",\"report_number\":\"2024-934604\",\"reporting_area\":\"1858\",\"sector\":\"W\",\"shooting_type_group\":\"-\"}]'\n\n\nLet‚Äôs load the schema toolset and play around with it.\n\nschema_tools = genai_mcp._load_raw_toolset(\"schema\")\n\nIt is going to show 1 tools again.\n\nlen(schema_tools)\n\n1\n\n\n\nschema_tools[0]('crime_data', 'simple')\n\n'[{\"object_details\":{\"name\":\"crime_data\"},\"object_name\":\"crime_data\",\"schema_name\":\"public\"}]'\n\n\n\nschema_tools[0]('crime_data', 'detailed')\n\n'[{\"object_details\":{\"columns\":[{\"column_comment\":null,\"column_default\":null,\"column_name\":\"report_number\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":1},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"report_datetime\",\"data_type\":\"timestamp without time zone\",\"is_not_nullable\":false,\"ordinal_position\":2},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"offense_id\",\"data_type\":\"bigint\",\"is_not_nullable\":false,\"ordinal_position\":3},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"offense_date\",\"data_type\":\"timestamp without time zone\",\"is_not_nullable\":false,\"ordinal_position\":4},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"nibrs_group_ab\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":5},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"nibrs_crime_against_category\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":6},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"offense_sub_category\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":7},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"shooting_type_group\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":8},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"block_address\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":9},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"latitude\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":10},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"longitude\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":11},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"beat\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":12},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"precinct\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":13},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"sector\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":14},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"neighborhood\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":15},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"reporting_area\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":16},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"offense_category\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":17},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"nibrs_offense_code_description\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":18},{\"column_comment\":null,\"column_default\":null,\"column_name\":\"nibrs_offense_code\",\"data_type\":\"text\",\"is_not_nullable\":false,\"ordinal_position\":19}],\"comment\":null,\"constraints\":[],\"indexes\":[],\"object_name\":\"crime_data\",\"object_type\":\"TABLE\",\"owner\":\"michaeltheologitis\",\"schema_name\":\"public\",\"triggers\":[]},\"object_name\":\"crime_data\",\"schema_name\":\"public\"}]'\n\n\nWe can also load the toolset used by OpenAI Agents (wrapped in tool funcs)\n\ngenai_mcp.load_toolset('sql')\n\n[FunctionTool(name='postgres_seattle_execute_sql', description='Executes SQL queries on the PostgreSQL Seattle database. The queries must be PostgreSQL-compatible.', params_json_schema={'properties': {'sql': {'description': 'The sql to execute.', 'title': 'Sql', 'type': 'string'}}, 'required': ['sql'], 'title': 'postgres_seattle_execute_sql_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=&lt;function function_tool.&lt;locals&gt;._create_function_tool.&lt;locals&gt;._on_invoke_tool&gt;, strict_json_schema=True, is_enabled=True, tool_input_guardrails=None, tool_output_guardrails=None)]\n\n\n\ngenai_mcp._load_raw_toolset('sql')\n\n[&lt;toolbox_core.sync_tool.ToolboxSyncTool&gt;]\n\n\n\ngenai_mcp.close()",
    "crumbs": [
      "Google's MCP Toolbox"
    ]
  },
  {
    "objectID": "agents.html",
    "href": "agents.html",
    "title": "Agents: Prompts, Schemas, and Orchestration",
    "section": "",
    "text": "We are going to first set the toolset to seattle. Practically, this means that our experts will subscribe to toolsets like seattle-sql, seattle-schema which you can find in the tools.yaml file at the root of the project.\ngenai_mcp.connect()\ngenai_mcp.set_specific_toolset('seattle')",
    "crumbs": [
      "Agents: Prompts, Schemas, and Orchestration"
    ]
  },
  {
    "objectID": "agents.html#expert-agent-data-expert",
    "href": "agents.html#expert-agent-data-expert",
    "title": "Agents: Prompts, Schemas, and Orchestration",
    "section": "Expert Agent: Data Expert",
    "text": "Expert Agent: Data Expert\nThe Data performs a high-level survey of available data sources without diving into detailed schemas.\n\nPrompt Definition:\n\n\nExported source\nDATA_EXPERT_PROMPT = \"\"\"\n# Role and Objective\nYou are a data expert that explores available sources. Your task is to perform a rapid, one-time survey of all accessible data sources. Your goal is to identify what databases, data stores, or connected sources exist and summarize, at a high level, what type of data each likely contains.\n\n# Exploration Scope\nFocus on high-level data assets only ‚Äî databases, schemas, APIs, or data files. Do not inspect or describe tables, columns, or detailed schemas. You are creating a top-down map of the data environment, not a deep schema inventory.\n\n# Grounding and Accuracy\nBase your findings only on verified information obtained via your tools. Never infer or imagine data that was not found. If a source cannot be accessed or no metadata is available, state that fact explicitly. Keep the exploration factual, concise, and high-level.\n\n# Instructions\n- Use your tools to discover and identify all accessible data sources as efficiently as possible.\n- Do not expand into schema-level details such as tables, columns, or keys.\n- Keep your report short, factual, and organized ‚Äî avoid long explanations or speculation.\n- Do not ask the user for clarification; make reasonable assumptions and proceed.\n- Prioritize speed and completeness of coverage over depth.\n\"\"\"\n\n\n\n\nInput/Output\nWe only have output here.\n\nOutput\nThis agent returns a DataReport containing a concise summary of discovered data sources.\n\ndescription(DataReport)\n\nreport: A concise single-paragraph report on the available data sources.\n\n\n\n\nexample_report = DataReport(\n    report=(\"[...] A nice concise description of the environment [...]\")\n)\n\n\nexample_report\n\nDataReport(report='[...] A nice concise description of the environment [...]')\n\n\n\n\n\nAgent-as-Tool Wrap\nHere, we expose the agent as a tool (i.e., discover_data_sources) to be used by the Lead Agent.\n\nsource\n\n\ndiscover_data_sources\n\n discover_data_sources ()\n\nPerforms a one-time high-level discovery of all connected data sources and returns a concise summary of available databases and their content domains.\n\n\nExample\n\nresult = await discover_data_sources()\n\n\nresult\n\nDataReport(report='I discovered one accessible data source: a PostgreSQL database named ‚ÄúSeattle‚Äù (public schema accessible). Within the public schema there is a dataset/object named ‚Äúcrime_data‚Äù (by name, it appears to be crime-related). No other databases, schemas, APIs, or data files were discoverable with the available tools or metadata.')",
    "crumbs": [
      "Agents: Prompts, Schemas, and Orchestration"
    ]
  },
  {
    "objectID": "agents.html#expert-agent-schema-expert",
    "href": "agents.html#expert-agent-schema-expert",
    "title": "Agents: Prompts, Schemas, and Orchestration",
    "section": "Expert Agent: Schema Expert",
    "text": "Expert Agent: Schema Expert\nThe Schema Expert answers questions about database structure, relationships, and schema design.\n\nPrompt/Instructions\n\n\nExported source\nSCHEMA_EXPERT_PROMPT = \"\"\"\n# Role and Objective\nYou are a database expert specializing in answering schema-related questions for relational databases. You have tools that allow you to inspect and analyze schemas across multiple databases. Use these tools whenever they are beneficial to your analysis.\n\n# Core Behavioral Principle\nNever invent or infer schemas, tables, or columns that are not actually present in the inspected databases. If no relevant database or table exists, state that explicitly and stop. Do not describe hypothetical, example, or canonical schemas under any circumstance.\n\n# Instructions\n- Do not ask the user for clarification. For ambiguous questions, make reasonable assumptions and include them at the end of your answer‚Äîalways present your answer first.\n- Identify which databases or data sources are relevant to the user's question.\n- Examine their schemas and explain how their structural elements (such as tables, columns, keys, and relationships) are connected.\n- Respond precisely to the user's intent, providing exactly the information requested‚Äînothing more, nothing less.\n- Always state explicitly the names of the databases your answer pertains to; it is imperative that the user knows the specific databases referenced.\n- Avoid speculation, assumptions, or irrelevant details.\n\"\"\"\n\n\n\n\nInput/Output\n\nInput\nThis agent expects a SchemaQuery.\n\ndescription(SchemaQuery)\n\ncontext_hint: A high-level hint about which database or domain the tool should focus on. This helps steer the tool toward the most relevant data sources.\n\nquery: The natural language request or question about the schema of the available relational databases.\n\n\n\n\nschema_query_example = SchemaQuery(\n    context_hint=\"[...] A high-level hint about which database or domain [...]\",\n    query=\"[...] The natural language request or question about the schema [...]\"\n)\n\n\nschema_query_example\n\nSchemaQuery(context_hint='[...] A high-level hint about which database or domain [...]', query='[...] The natural language request or question about the schema [...]')\n\n\n\n\nOutput\nThe Schema Expert returns a SchemaQueryAnswer object.\n\ndescription(SchemaQueryAnswer)\n\nanswer: The final synthesized answer to the user's schema-related question. The response must explicitly state the names of all databases it pertains to.\n\n\n\n\nschema_answer_example = SchemaQueryAnswer(\n    answer=\"[...] The final answer to the schema-related question [...]\"\n)\n\n\nschema_answer_example\n\nSchemaQueryAnswer(answer='[...] The final answer to the schema-related question [...]')\n\n\n\n\n\nAgent-as-Tool Wrap\nHere, we expose the agent as a tool (i.e., schema_query) to be used by the Lead Agent.\n\nsource\n\n\nschema_query\n\n schema_query (query:__main__.SchemaQuery)\n\nA tool that answers natural language questions about database schemas across multiple potential relational database sources. It can describe tables, columns, relationships, keys, and other structural details for the relevant database.\n\n\n\n\nType\nDetails\n\n\n\n\nquery\nSchemaQuery\nThe SchemaQuery instance containing the user‚Äôs schema question\n\n\nReturns\nSchemaQueryAnswer\nThe SchemaQueryAnswer instance containing the final answer\n\n\n\n\n\nExample\n\ns = SchemaQuery(\n    context_hint=\"Seattle, WA\",\n    query=\"Are there any columns related to dates of crimes?\"\n)\n\nresult = await schema_query(s)\n\n\nresult\n\nSchemaQueryAnswer(answer='Database: Seattle\\n\\nTable: public.crime_data\\n- offense_date ‚Äî timestamp without time zone\\n- report_datetime ‚Äî timestamp without time zone\\n\\nNo other tables or columns in the Seattle database schema contain date/time data according to the inspected schema.\\n\\nAssumption: You meant columns that store date/time values related to crimes in the Seattle database; I inspected all tables in that database.')\n\n\n\nprint(result.answer)\n\nDatabase: Seattle\n\nTable: public.crime_data\n- offense_date ‚Äî timestamp without time zone\n- report_datetime ‚Äî timestamp without time zone\n\nNo other tables or columns in the Seattle database schema contain date/time data according to the inspected schema.\n\nAssumption: You meant columns that store date/time values related to crimes in the Seattle database; I inspected all tables in that database.",
    "crumbs": [
      "Agents: Prompts, Schemas, and Orchestration"
    ]
  },
  {
    "objectID": "agents.html#expert-agent-sql-expert",
    "href": "agents.html#expert-agent-sql-expert",
    "title": "Agents: Prompts, Schemas, and Orchestration",
    "section": "Expert Agent: SQL Expert",
    "text": "Expert Agent: SQL Expert\nhe SQL Expert translates NL questions into SQL queries and returns the evidence (in SQL) for its answers.\n\nPrompt/Instructions\n\n\nExported source\nSQL_EXPERT_PROMPT = \"\"\"\n# Role and Objective\nYou are an SQL expert focused on transparency and reproducibility. Your goal is to answer the user's question as accurately and directly as possible, while *always displaying every final SQL query* that contributed evidence to your answer. Each part of your response must clearly show the concrete SQL query (or queries) that produced the corresponding result.  You have access to tools that allow you to execute SQL queries on various databases. Use these tools whenever they are beneficial to your analysis.\n\n# Evidence Traceability\n- For each analytical statement or conclusion you present, include the exact SQL query that generated the supporting data. \n- Only show SQL queries that were successfully executed and directly used to form your final answer. \n- Do not show intermediate or failed queries. \n- When multiple queries are used (for different sub-parts of the reasoning), display each query alongside the reasoning it supports, in clearly labeled sections.\n\n# Instructions\n- Do not ask the user for clarification. For ambiguous questions, make reasonable assumptions and include them at the end of your answer ‚Äî always present your answer first.\n- Translate natural language questions into SQL queries, execute them, and communicate the results clearly in natural language.\n- Before executing any SQL query, verify that it is well-defined and addresses a single, specific information need.\n- For multi-step problems, plan the sequence of steps explicitly and execute them sequentially, integrating each intermediate result into the final coherent answer.\n- Ensure each SQL query matches the SQL dialect of the target database used by the query tool.\n\n# SQL Query Best Practices\n- Plan your approach before executing any query. \n- Prefer multiple simple, well-scoped queries over single complex ones by breaking problems into logical sub-steps.\n\"\"\"\n\n\n\n\nInput/Output\n\nInput\nThis agent expects a NLQuery (we will later re-structure it manually in the orchestration module).\n\ndescription(NLQuery)\n\nquery: The user's request or question expressed in natural language.\n\nschema_info: The relevant database schema information, including the names of all databases involved, as well as details on tables, relationships, and foreign keys necessary for answering the query.\n\n\n\n\nnl_query_example = NLQuery(\n    query=\"[...] The user's request or question [...]\",\n    schema_info=\"[...] The relevant database schema information [...]\"\n)\n\n\nprint(nl_query_example)\n\nquery=\"[...] The user's request or question [...]\" schema_info='[...] The relevant database schema information [...]'\n\n\nSince the LLMs expect NL inputs, we must structure multi-variable schemas accordingly:\n\nprint(nl_query_example.describe_for_agent())\n\n### NL Request or Question\n[...] The user's request or question [...]\n### Relevant Schema Information for the Necessary Data\n[...] The relevant database schema information [...]\n\n\n\n\n\nOutput\nThe SQL Expert returns a NLQueryAnswer. This is a common structure we will use again.\n\ndescription(NLQueryAnswer)\n\nanswer: The final synthesized answer to the user's natural language query, expressed clearly and completely in natural language.\n\n\n\n\nnl_query_answer = NLQueryAnswer(\n    answer=\"[...] The final answer to a user's query (the 'user' can be an agent ofc) [...]\"\n)\n\n\nprint(nl_query_answer)\n\nanswer=\"[...] The final answer to a user's query (the 'user' can be an agent ofc) [...]\"\n\n\n\n\n\nAgent-as-Tool Wrap\nHere, we expose the agent as a tool (i.e., nl_query) to be used by the Lead Agent.\n\nsource\n\n\nnl_query\n\n nl_query (query:__main__.NLQuery)\n\nHandles natural language questions by translating them into SQL, executing the queries, and returning both the results and the concrete SQL statements that produced them. Each part of the answer is accompanied by the exact executed SQL query that served as its evidence.\n\n\n\n\nType\nDetails\n\n\n\n\nquery\nNLQuery\nThe NLQuery instance containing the user‚Äôs question and schema info\n\n\nReturns\nNLQueryAnswer\nThe NLQueryAnswer instance containing the final answer and SQL evidence\n\n\n\n\n\nExample\n\nq = NLQuery(\n    query=\"How many unique police sectors are there? Could you list them with counts?\",\n    schema_info=\"Seattle database, columns: beat, sector\"\n)\n\nresult = await nl_query(q)\n\n\nprint(result.answer)\n\nAnswer:\n\n- Number of unique police sectors: 20\n\n- List of sectors with counts (sector : count):\n  U : 114,581\n  E : 108,678\n  K : 107,441\n  B : 104,270\n  M : 101,570\n  D : 100,808\n  R : 95,004\n  Q : 91,649\n  L : 87,061\n  N : 85,461\n  S : 76,985\n  W : 75,603\n  J : 72,777\n  F : 72,619\n  C : 66,903\n  G : 66,803\n  O : 47,993\n  - : 11,072\n  99 : 3,544\n  OOJ : 176\n\nSQL queries executed (each query below produced the stated result):\n\n1) Query used to locate table(s) containing the columns \"sector\" or \"beat\":\nSELECT table_schema, table_name, column_name\nFROM information_schema.columns\nWHERE column_name IN ('sector','beat')\nORDER BY table_schema, table_name;\n\nResult returned:\n  table_schema: public, table_name: crime_data, column_name: beat\n  table_schema: public, table_name: crime_data, column_name: sector\n\n2) Query to count distinct sectors:\nSELECT COUNT(DISTINCT sector) AS unique_sectors\nFROM public.crime_data;\n\nResult returned:\n  unique_sectors: 20\n\n3) Query to list each sector with its count (ordered by count desc):\nSELECT sector, COUNT(*) AS count\nFROM public.crime_data\nGROUP BY sector\nORDER BY count DESC, sector;\n\nResult returned (sector, count):\n  (\"U\", 114581)\n  (\"E\", 108678)\n  (\"K\", 107441)\n  (\"B\", 104270)\n  (\"M\", 101570)\n  (\"D\", 100808)\n  (\"R\", 95004)\n  (\"Q\", 91649)\n  (\"L\", 87061)\n  (\"N\", 85461)\n  (\"S\", 76985)\n  (\"W\", 75603)\n  (\"J\", 72777)\n  (\"F\", 72619)\n  (\"C\", 66903)\n  (\"G\", 66803)\n  (\"O\", 47993)\n  (\"-\", 11072)\n  (\"99\", 3544)\n  (\"OOJ\", 176)\n\nNotes and assumptions (presented after the answer):\n- I searched information_schema.columns to find which table(s) in the Seattle database contain the columns \"sector\" and \"beat\"; that returned a single table: public.crime_data. All subsequent queries used public.crime_data.\n- The sector values include some non-alphabetic entries (for example '-', '99', 'OOJ'); I reported them as they appear in the data.\n- If you want the list filtered to only standard sector codes (e.g., exclude '-', '99', 'OOJ') or sorted differently, tell me which filter/sort you prefer and I will run the adjusted query and show the exact SQL.",
    "crumbs": [
      "Agents: Prompts, Schemas, and Orchestration"
    ]
  },
  {
    "objectID": "agents.html#lead-agent-verifier",
    "href": "agents.html#lead-agent-verifier",
    "title": "Agents: Prompts, Schemas, and Orchestration",
    "section": "Lead Agent: Verifier",
    "text": "Lead Agent: Verifier\nThe Verifier is the orchestrating agent that verifies claims by coordinating with other expert agents and grounding all conclusions in real data.\n\nPrompt/Instructions\n\n\nExported source\nVERIFIER_PROMPT = \"\"\"\n# Role and Objective\nYou are a data expert and a verifier. Your goal is to verify every claim provided to you by grounding your reasoning in real, verifiable data sources. You have access to various tools that enable you to retrieve and analyze factual data‚Äîuse them whenever they enhance your analysis. You must produce a clear and structured report that summarizes your findings and **includes the exact SQL queries** that generated the supporting evidence. Your query tools are specifically designed to return these executed SQL statements for inclusion in your report.\n\n# Data Grounding Principles\n- Always begin by exploring the available data sources to understand their structure and contents before interpreting the claim. This ensures your reasoning is firmly grounded in the real data environment.\n- Treat all accessible data sources as **reliable and authoritative**. You can fully trust that the data you access is accurate, complete within its scope, and suitable for verification.\n- Base your conclusions strictly on what the data supports. Avoid speculation or reasoning not grounded in evidence from the data.\n\n# Instructions\n- You should always base your conclusions on real data.\n- Do not ask the user for clarification. For ambiguous questions, first explore the available data environment to ground your interpretation in what the data represents. Then make reasonable assumptions about the claim's intent and clearly list them at the end of your report‚Äîafter providing your answer.\n- Present the collected evidence directly in your report‚Äîincluding any executed SQL‚Äîensuring that each conclusion is visibly grounded in data.\n- Verify each user claim by consulting available data sources.\n- Examine claims thoroughly and assess whether they are supported or contradicted by the evidence.\n- Use tools to obtain the information you need, delegating clear and well-scoped tasks to them when appropriate.\n- For multi-step questions, plan the reasoning explicitly and execute each step through a separate tool call. Each call should address one specific information need.\n- Remember: Tools are stateless. Recreate any necessary context between tool calls explicitly.\n\"\"\"\n\n\n\n\nInput/Output\n\nInput\nThis agent expects a NL UserQuery.\n\nuser_query_example = UserQuery(\n    query=\"[...] The user's request or question [...]\"\n)\n\n\nuser_query_example\n\nUserQuery(query=\"[...] The user's request or question [...]\")\n\n\n\n\nOutput\nThe Verifier returns a VerificationAnswer object.\n\ndescription(VerificationAnswer)\n\nreport: The full report describing which parts of the claims are true and which are not.\n\nverdict: Your final verdict should be one of the following:\n- **Verified**: The overall claim is fully supported by the evidence, allowing for minor acceptable deviations (e.g., rounding, naming, or formatting differences).\n- **Partly Verified**: The overall claim is supported by the evidence, but some supporting details are incomplete, imprecise, or contain minor factual inaccuracies.\n- **Partly Inaccurate**: The overall claim contains a mixture of true and false elements, with errors substantial enough to undermine confidence in the conclusion.\n- **Inaccurate**: The overall claim is contradicted or unsupported by the evidence.\n\n\n\n\n\nverification_answer_example = VerificationAnswer(\n    report=\"[...] A detailed report on the verification of the claims with concrete SQL [...]\",\n    verdict=\"Verified\"\n)\n\n\nverification_answer_example\n\nVerificationAnswer(report='[...] A detailed report on the verification of the claims with concrete SQL [...]', verdict='Verified')\n\n\n\n\n\nExample\n\nsource\n\n\nverify\n\n verify (user_query:__main__.UserQuery)\n\n\n\n\n\nType\nDetails\n\n\n\n\nuser_query\nUserQuery\nThe UserQuery instance containing the user‚Äôs claims to verify):\n\n\n\n\nq = UserQuery(query=\"Seattle has less than 10 unique police sectors.\")\n\nresult = await verify(q)\n\n\nprint(result.verdict)\n\nInaccurate\n\n\n\nprint(result.report)\n\nData sources discovered\n- Seattle database (public schema). One key table relevant to police geography was identified: public.crime_data, which includes a sector column.\n\nSchema exploration\n- Table: public.crime_data ‚Äî contains incident-level records with police geography fields: beat (text), precinct (text), sector (text), reporting_area (text), neighborhood (text).\n\nEvidence from executed SQL\n1) List all distinct sectors\nSQL executed:\nSELECT DISTINCT sector FROM public.crime_data ORDER BY sector;\nResult:\n- \"-\"\n- \"99\"\n- \"B\"\n- \"C\"\n- \"D\"\n- \"E\"\n- \"F\"\n- \"G\"\n- \"J\"\n- \"K\"\n- \"L\"\n- \"M\"\n- \"N\"\n- \"O\"\n- \"OOJ\"\n- \"Q\"\n- \"R\"\n- \"S\"\n- \"U\"\n- \"W\"\n\n2) Count distinct sectors\nSQL executed:\nSELECT COUNT(DISTINCT sector) AS unique_sector_count FROM public.crime_data;\nResult:\n- unique_sector_count: 20\n\nAnalysis\n- The dataset records 20 distinct sector values (non-null), which is not less than 10.\n\nConclusion\n- The claim that ‚ÄúSeattle has less than 10 unique police sectors‚Äù is contradicted by the data in the Seattle database public.crime_data table.\n\nAssumptions and notes\n- The count includes all distinct non-null values in the sector field as stored, including placeholder-like values such as '-' and '99' if present in the source. Even if such placeholders were excluded, the remaining lettered sectors alone exceed 10, so the conclusion remains unchanged.\n\n\n\ngenai_mcp.close()",
    "crumbs": [
      "Agents: Prompts, Schemas, and Orchestration"
    ]
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Main",
    "section": "",
    "text": "source\n\nmain\n\n main ()",
    "crumbs": [
      "Main"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thucy: An LLM-based MAS for Claim Verification",
    "section": "",
    "text": "The architecture of Thucy\nIn today‚Äôs age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims‚Äîoften about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases‚Äîtypically a few hundred rows‚Äîthat conveniently fit within an LLM‚Äôs context window.\nIn this project, we propose Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset‚Äîthe standard benchmark for fact verification over structured data‚ÄîThucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs.¬†88.7%).\nWe highly encourage to read the docs from the official Thucy website (complete, and better rendering).",
    "crumbs": [
      "Thucy: An LLM-based MAS for Claim Verification"
    ]
  },
  {
    "objectID": "index.html#usage-example",
    "href": "index.html#usage-example",
    "title": "Thucy: An LLM-based MAS for Claim Verification",
    "section": "Usage Example",
    "text": "Usage Example\nGiven the daily-updated Seattle crime data (1.5GB) from the City of Seattle, we set out to verify the claim:\n\nThe number of violent crimes decreased in Seattle between 2024 and 2023.\n\nWe can simply load the data into a database and ask Thucy to verify the claim:\nthucy verify \"The number of violent crimes decreased in Seattle between 2024 and 2023\" --workflow \"Violent-Crimes\" --toolset seattle\n\n\nSetting up connection to Google's toolbox...\nSuccess!\nStarting Verification. Might take a while...!\nSuccess!\n\nThucy Verdict: Inaccurate\n\nSaved full report to experiments/results/Violent-Crimes-trace_824464db51d741d196726bed0316f034.txt\nExiting!",
    "crumbs": [
      "Thucy: An LLM-based MAS for Claim Verification"
    ]
  },
  {
    "objectID": "index.html#clone",
    "href": "index.html#clone",
    "title": "Thucy: An LLM-based MAS for Claim Verification",
    "section": "Clone",
    "text": "Clone\nClone from GitHub:\ngit clone https://github.com/michaeltheologitis/thucy.git\nGo to the project directory:\ncd thucy\nThen, install the package (usually in a virtual environment):\npip install -e .",
    "crumbs": [
      "Thucy: An LLM-based MAS for Claim Verification"
    ]
  },
  {
    "objectID": "index.html#configurations",
    "href": "index.html#configurations",
    "title": "Thucy: An LLM-based MAS for Claim Verification",
    "section": "Configurations",
    "text": "Configurations\nWe know that setting up and configuring stuff is a pain. However, there are a lot of moving parts in this project. We have made the configuration as easy as possible from the user-side ‚Äî we take care of most of the heavy lifting for you.\nThe only thing we need to configure manually are the specific database connection details (usernames, passwords, etc.) so that we can deploy Google‚Äôs MCP Toolbox, and also the OpenAI API keys for using the OpenAI Agents SDK. We will walk you through the steps to do that. It will be quick and easy, we promise!\n\nOpenAI API Key\nTo use Thucy, you need to have an OpenAI API key. If you don‚Äôt have one, you can sign up at OpenAI‚Äôs website and create an API key.\nIn order to set an environment variable in Thucy we simply run the following:\nthucy config set OPENAI_API_KEY sk_xxyy\nwhere sk_xxyy is your actual OpenAI API key.\nThen, you can verify that it has been set correctly by inspecting the configuration:\nthucy config show\n\n\n\nOPENAI_API_KEY=sk-xxyy\nEXPERTS_MODEL=gpt-5-mini\nLEAD_MODEL=gpt-5\nSQL_EXPERT_MAX_TURNS=30\nSCHEMA_EXPERT_MAX_TURNS=30\nDATA_EXPERT_MAX_TURNS=30\nGENAI_SERVER_URL=http://127.0.0.1:5000\nLEAD_MAX_TURNS=40\n\n\n\n\n\nGoogle‚Äôs MCP Toolbox\n\nDatabases\nGoogle makes our life easy because they built a great tool for managing database connections and agentic tools. If you want to learn more you can read the blog.\nHere, all we care about is setting up the database connections properly and defining a few tools! We will only work with the tools.yaml file. Let‚Äôs take a peak on how the first lines look like:\nsources:\n  postgres-seattle:  # A custom name of our choice!\n    database: seattle   # The actual database name \n    host: localhost\n    kind: postgres  # The type of database\n    password: guest_pass\n    port: 5432\n    user: guest_user\nUnder the sources section, we define all our database connections. In this example, we have a PostgreSQL database named seattle running on our local machine. We also have a postgres-defined user guest_user with password guest_pass. This is just an example, you should replace these values with your actual database connection details.\nTips: ‚ë† Input a user that has all permissions; ‚ë° Always have a password (do not leave this empty or delete it).\nWe follow the documentation found here. Of course, we can add more database connections (e.g., MySQL, SQLite, etc.) by following the same pattern as above. We simply append more entries under the sources section. Follow the link above for other database connections and their required parameters (e.g., MySQL, SQLite, etc.).\n\n\nTools\nNow, we are ready to go to the fun stuff. Let‚Äôs take a peak again to the next lines of tools.yaml:\ntools:\n  postgres_seattle_execute_sql:  # Custom name!\n    description: Executes SQL queries on the PostgreSQL Seattle database. The queries must be PostgreSQL-compatible.\n    kind: postgres-execute-sql\n    source: postgres-seattle\n  postgres_seattle_list_tables:\n    description: Retrieves PostgreSQL schema information in the Seattle database. Supports both **simple** (table names only) and **detailed** output.\n    kind: postgres-list-tables  # This is Google's primitive (like above)!\n    source: postgres-seattle\nWe defined two tools: ‚ë† postgres_seattle_execute_sql, ‚ë° postgres_seattle_list_tables. Notice that they are binded to the Seattle database (i.e., source: postgres-seattle - this is the exact name we gave our database connection above!). Then, we also bind the tool to a Google‚Äôs primitive function (like postgres-execute-sql and postgres-list-tables, see here for their definition).\nAs long as you did not change the name of the database connection (i.e., postgres-seattle) you can keep this configuration as is! üòá\nOf course, you can create a lot more tools for different databases by following the same pattern.\n\n\nToolsets\nHere, we will bring it all together. Let‚Äôs take a peak at the final lines of tools.yaml:\ntoolsets:\n  seattle-schema:  # Custom name! BUT, our system expects `-schema` suffix for schema toolsets!\n  - postgres_seattle_list_tables\n  seattle-sql:  # Custom name! BUT, our system expects `-sql` suffix for SQL toolsets!\n  - postgres_seattle_execute_sql\nWe can define toolsets that group related tools together. In this example, we have two toolsets: seattle-schema and seattle-sql. Of course, in this example we only have one tool in each toolset.\nThis is not how it usually is. We encourage you to take a look at tools.yaml.template for more complex examples with multiple databases and tools. This is where you will appreciate the flexibility of Google‚Äôs toolbox!\n\n\nRunning the MCP Toolbox\nNow, we are ready to run toolbox. Send it:\ntoolbox --ui\n\n\n2025-12-02T19:36:39.07939-08:00 INFO \"Initialized 1 sources.\" \n2025-12-02T19:36:39.079447-08:00 INFO \"Initialized 0 authServices.\" \n2025-12-02T19:36:39.079637-08:00 INFO \"Initialized 2 tools.\" \n2025-12-02T19:36:39.079648-08:00 INFO \"Initialized 3 toolsets.\" \n2025-12-02T19:36:39.079846-08:00 INFO \"Server ready to serve!\" \n2025-12-02T19:36:39.079852-08:00 INFO \"Toolbox UI is up and running at: http://127.0.0.1:5000/ui\n\n\nCongratulations! We have completed the configurations. üéâ\nIf you encounter errors, double-check for indentation mistakes (talking from personal experience here). Usually, the errors are self-explanatory and easily-fixable. Otherwise, you can take a look at the docs.",
    "crumbs": [
      "Thucy: An LLM-based MAS for Claim Verification"
    ]
  },
  {
    "objectID": "index.html#add-some-interesting-data",
    "href": "index.html#add-some-interesting-data",
    "title": "Thucy: An LLM-based MAS for Claim Verification",
    "section": "Add some Interesting Data!",
    "text": "Add some Interesting Data!\nRemember to add some data to your database(s). For example, Seattle Crime Data or Los Angeles Crime etc. Simply COPY the CSV files into the database(s) you configured above. Do not bother with the messiness of the data, Thucy‚Äôs job is to handle that!",
    "crumbs": [
      "Thucy: An LLM-based MAS for Claim Verification"
    ]
  },
  {
    "objectID": "index.html#usage-workflow",
    "href": "index.html#usage-workflow",
    "title": "Thucy: An LLM-based MAS for Claim Verification",
    "section": "Usage Workflow",
    "text": "Usage Workflow\nFirst, make sure that the toolbox is running:\ntoolbox --ui\nThen, we can run Thucy as follows:\nthucy verify &lt;CLAIM-TO-VERIFY&gt; --workflow &lt;CUSTOM-NAME&gt; --toolset &lt;TOOLSET-TO-USE&gt;\nThe &lt;CUSTOM-NAME&gt; is an arbitrary name you give to your workflow (e.g., Violent-Crimes in the example above).\nThe &lt;TOOLSET-TO-USE&gt; is a prefix of the toolset you want to use from the tools.yaml we configured above. Internally, Thucy always uses two toolsets in exacution: ‚ë† a schema toolset, and a ‚ë° a sql toolset. Users are expected to give the common prefix of these toolsets. In our configuration example above, this would be seattle. What happens in the code is that we assign seattle-sql to the SQL expert agent, and seattle-schema to the SQL expert agent!\nWe also encourage you to take a look at the tools.yaml.template configuration which showcases the flexibility of the toolbox. In this configuration we might choose to do --toolset west-coast. This would give the agents access to 3 databases.",
    "crumbs": [
      "Thucy: An LLM-based MAS for Claim Verification"
    ]
  }
]