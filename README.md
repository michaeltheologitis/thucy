# Thucy: An LLM-based MAS for Claim Verification across Databases


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

<div align="center">

[![](https://img.shields.io/badge/paper-A42C25?style=for-the-badge&logo=arxiv&logoColor=white.png)](https://arxiv.org/abs/2512.03278)

</div>

![The architecture of
Thucy](https://raw.githubusercontent.com/michaeltheologitis/thucy/main/docs/images/thucy.png)

In this project, we propose **Thucy**, the first cross-database,
cross-table multi-agent claim verification system that also provides
concrete evidence for each verification verdict. Thucy remains
**completely agnostic to the underlying data sources** before deployment
and must therefore autonomously discover, inspect, and reason over all
available relational databases to verify claims. Importantly, Thucy also
reports the exact SQL queries that support its verdict (whether the
claim is accurate or not) offering full transparency to expert users
familiar with SQL. When evaluated on the TabFact datasetâ€”the standard
benchmark for fact verification over structured dataâ€”Thucy surpasses the
previous state of the art by 5.6 percentage points in accuracy (94.3%
vs.Â 88.7%).

We highly encourage to read the docs from the official [Thucy
website](https://michaeltheologitis.github.io/thucy/) (complete, and
better rendering).

## Usage Example

Given the daily-updated Seattle [crime
data](https://data.seattle.gov/Public-Safety/SPD-Crime-Data-2008-Present/tazs-3rd5/about_data)
(1.5GB) from the City of Seattle, we set out to verify the claim:

> The number of violent crimes decreased in Seattle between 2024 and
> 2023.

We can simply load the data into a database and ask Thucy to verify the
claim:

``` sh
thucy verify "The number of violent crimes decreased in Seattle between 2024 and 2023" --workflow "Violent-Crimes" --toolset seattle
```

    Setting up connection to Google's toolbox...
    Success!
    Starting Verification. Might take a while...!
    Success!

    Thucy Verdict: Inaccurate

    Saved full report to experiments/results/Violent-Crimes-trace_824464db51d741d196726bed0316f034.txt
    Exiting!

# Installation

## Clone

Clone from [GitHub](https://github.com/michaeltheologitis/thucy):

``` sh
git clone https://github.com/michaeltheologitis/thucy.git
```

Go to the project directory:

``` sh
cd thucy
```

Then, install the package (usually in a virtual environment):

``` sh
pip install -e .
```

## Configurations

We know that setting up and configuring stuff is a pain. However, there
are a lot of moving parts in this project. We have made the
configuration **as easy as possible** from the user-side â€” we take care
of most of the heavy lifting for you.

The only thing we need to configure manually are the specific database
connection details (usernames, passwords, etc.) so that we can deploy
Googleâ€™s MCP Toolbox, and also the OpenAI API keys for using the OpenAI
Agents SDK. We will walk you through the steps to do that. **It will be
quick and easy, we promise!**

### OpenAI API Key

To use Thucy, you need to have an OpenAI API key. If you donâ€™t have one,
you can sign up at [OpenAIâ€™s
website](https://platform.openai.com/api-keys) and create an API key.

In order to set an environment variable in Thucy we simply run the
following:

``` sh
thucy config set OPENAI_API_KEY sk_xxyy
```

where `sk_xxyy` is your actual OpenAI API key.

Then, you can verify that it has been set correctly by inspecting the
configuration:

``` sh
thucy config show
```


    OPENAI_API_KEY=sk-xxyy
    EXPERTS_MODEL=gpt-5-mini
    LEAD_MODEL=gpt-5
    SQL_EXPERT_MAX_TURNS=30
    SCHEMA_EXPERT_MAX_TURNS=30
    DATA_EXPERT_MAX_TURNS=30
    GENAI_SERVER_URL=http://127.0.0.1:5000
    LEAD_MAX_TURNS=40

### Googleâ€™s MCP Toolbox

#### Databases

Google makes our life easy because they built a great tool for managing
database connections and agentic **tools**. If you want to learn more
you can read the
[blog](https://cloud.google.com/blog/products/ai-machine-learning/mcp-toolbox-for-databases-now-supports-model-context-protocol).

Here, all we care about is setting up the database connections properly
and defining a few tools! We will only work with the `tools.yaml` file.
Letâ€™s take a peak on how the first lines look like:

``` yaml
sources:
  postgres-seattle:  # A custom name of our choice!
    database: seattle   # The actual database name 
    host: localhost
    kind: postgres  # The type of database
    password: guest_pass
    port: 5432
    user: guest_user
```

Under the `sources` section, we define all our database connections. In
this example, we have a PostgreSQL database named `seattle` running on
our local machine. We also have a postgres-defined user `guest_user`
with password `guest_pass`. This is just an example, **you should
replace these values with your actual database connection details.**

Tips: â‘  Input a user that has all permissions; â‘¡ Always have a
`password` (do not leave this empty or delete it).

We follow the documentation found
[here](https://googleapis.github.io/genai-toolbox/resources/sources/postgres/).
Of course, we can add more database connections (e.g., MySQL, SQLite,
etc.) by following the same pattern as above. We simply append more
entries under the `sources` section. Follow the link above for other
database connections and their required parameters (e.g.,
[MySQL](https://googleapis.github.io/genai-toolbox/resources/sources/mysql/),
[SQLite](https://googleapis.github.io/genai-toolbox/resources/sources/sqlite/),
etc.).

#### Tools

Now, we are ready to go to the fun stuff. Letâ€™s take a peak again to the
next lines of `tools.yaml`:

``` yaml
tools:
  postgres_seattle_execute_sql:  # Custom name!
    description: Executes SQL queries on the PostgreSQL Seattle database. The queries must be PostgreSQL-compatible.
    kind: postgres-execute-sql
    source: postgres-seattle
  postgres_seattle_list_tables:
    description: Retrieves PostgreSQL schema information in the Seattle database. Supports both **simple** (table names only) and **detailed** output.
    kind: postgres-list-tables  # This is Google's primitive (like above)!
    source: postgres-seattle
```

We defined two tools: â‘  `postgres_seattle_execute_sql`, â‘¡
`postgres_seattle_list_tables`. Notice that they are *binded* to the
Seattle database (i.e., `source: postgres-seattle` - this is the exact
name we gave our database connection above!). Then, we also *bind* the
tool to a Googleâ€™s primitive function (like `postgres-execute-sql` and
`postgres-list-tables`, see
[here](https://googleapis.github.io/genai-toolbox/resources/sources/postgres/)
for their definition).

As long as you did not change the name of the database connection (i.e.,
`postgres-seattle`) you can keep this configuration as is! ðŸ˜‡

Of course, you can create a lot more tools for different databases by
following the same pattern.

#### Toolsets

Here, we will bring it all together. Letâ€™s take a peak at the final
lines of `tools.yaml`:

``` yaml
toolsets:
  seattle-schema:  # Custom name! BUT, our system expects `-schema` suffix for schema toolsets!
  - postgres_seattle_list_tables
  seattle-sql:  # Custom name! BUT, our system expects `-sql` suffix for SQL toolsets!
  - postgres_seattle_execute_sql
```

We can define **toolsets** that group related tools together. In this
example, we have two toolsets: `seattle-schema` and `seattle-sql`. Of
course, in this example we only have one tool in each toolset.

This is **not** how it usually is. We encourage you to take a look at
`tools.yaml.template` for more complex examples with multiple databases
and tools. This is where you will appreciate the flexibility of Googleâ€™s
toolbox!

#### Running the MCP Toolbox

Now, we are ready to run toolbox. Send it:

``` sh
toolbox --ui
```

    2025-12-02T19:36:39.07939-08:00 INFO "Initialized 1 sources." 
    2025-12-02T19:36:39.079447-08:00 INFO "Initialized 0 authServices." 
    2025-12-02T19:36:39.079637-08:00 INFO "Initialized 2 tools." 
    2025-12-02T19:36:39.079648-08:00 INFO "Initialized 3 toolsets." 
    2025-12-02T19:36:39.079846-08:00 INFO "Server ready to serve!" 
    2025-12-02T19:36:39.079852-08:00 INFO "Toolbox UI is up and running at: http://127.0.0.1:5000/ui

Congratulations! We have completed the configurations. ðŸŽ‰

If you encounter errors, double-check for indentation mistakes (talking
from personal experience here). Usually, the errors are self-explanatory
and easily-fixable. Otherwise, you can take a look at the
[docs](https://googleapis.github.io/genai-toolbox/resources/sources/).

## Add some Interesting Data!

Remember to add some data to your database(s). For example, [Seattle
Crime
Data](https://data.seattle.gov/Public-Safety/SPD-Crime-Data-2008-Present/tazs-3rd5/about_data)
or [Los Angeles
Crime](https://catalog.data.gov/dataset/crime-data-from-2020-to-present)
etc. Simply `COPY` the CSV files into the database(s) you configured
above. **Do not bother with the messiness of the data, Thucyâ€™s job is to
handle that!**

## Usage Workflow

First, make sure that the toolbox is running:

``` sh
toolbox --ui
```

Then, we can run Thucy as follows:

``` sh
thucy verify <CLAIM-TO-VERIFY> --workflow <CUSTOM-NAME> --toolset <TOOLSET-TO-USE>
```

The `<CUSTOM-NAME>` is an arbitrary name you give to your workflow
(e.g., `Violent-Crimes` in the example above).

The `<TOOLSET-TO-USE>` is a prefix of the toolset you want to use from
the `tools.yaml` we configured above. Internally, Thucy always uses two
toolsets in exacution: â‘  a schema toolset, and a â‘¡ a sql toolset. Users
are expected to give the common prefix of these toolsets. In our
configuration example above, this would be `seattle`. What happens in
the code is that we assign `seattle-sql` to the SQL expert agent, and
`seattle-schema` to the SQL expert agent!

We also encourage you to take a look at the `tools.yaml.template`
configuration which showcases the flexibility of the toolbox. In this
configuration we might choose to do `--toolset west-coast`. This would
give the agents access to 3 databases.

# Paper Results

In order to vizualise the results of the paper (or reproduce them from
scratch), please run the notebook `experiments/paper/tabfact.ipynb`.

# Extend & Develop

This project has been created using [nbdev](https://nbdev.fast.ai/) (big
shoutout!).

If you want to contribute first run:

``` sh
nbdev_install_quarto
nbdev_install_hooks
```
